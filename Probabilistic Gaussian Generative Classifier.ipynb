{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b67c2cbb",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5961111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from numpy.linalg import slogdet, inv\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd4a7b5",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e90c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = sklearn.datasets.load_digits()\n",
    "## load the digits dataset from sklearn\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "## Create a 70% / 15% / 15% split:\n",
    "X_train, X_temp, y_train, y_temp = sklearn.model_selection.train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = sklearn.model_selection.train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "##  standardize the features\n",
    "scaler=StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_val=scaler.transform(X_val)\n",
    "X_test=scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938be637",
   "metadata": {},
   "source": [
    "## Gaussian Generative Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546f8b8d",
   "metadata": {},
   "source": [
    "### 1. compute class priors pi_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e51fd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class priors: [0.09864757 0.10103421 0.09864757 0.10182975 0.10103421 0.10103421\n",
      " 0.10103421 0.09944312 0.09705648 0.10023866]\n"
     ]
    }
   ],
   "source": [
    "#pi_k = P(y=k)\n",
    "K= 10\n",
    "N= X_train.shape[0] \n",
    "pi = np.zeros(K)\n",
    "for k in range(K):\n",
    "    pi[k] = np.sum(y_train == k) / N\n",
    "\n",
    "print(\"Class priors:\", pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a734d4",
   "metadata": {},
   "source": [
    "### 2. compute class means mu_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29747f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class means shape: (10, 64)\n"
     ]
    }
   ],
   "source": [
    "# mu_k = E[x|y=k]\n",
    "D= X_train.shape[1]\n",
    "mu = np.zeros((K,D))\n",
    "for k in range(K):\n",
    "    mu[k] = np.mean(X_train[y_train == k], axis=0) # x given y=k \n",
    "\n",
    "print(\"Class means shape:\", mu.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f919360a",
   "metadata": {},
   "source": [
    "### 3. compute shared covariance Σ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eb44b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared covariance shape: (64, 64)  trace: 41.085022049776384\n"
     ]
    }
   ],
   "source": [
    "# sigma= 1/n * sum_i (x_i - mu_(y_i))(x_i - mu_(y_i))^T\n",
    "sigma = np.zeros((D,D))\n",
    "for i in range(N):\n",
    "    diff= (X_train[i] - mu[y_train[i]])[:,None] # make it a column vector (D x 1)\n",
    "    sigma += diff.dot(diff.T)\n",
    "sigma /= N\n",
    "print(\"Shared covariance shape:\", sigma.shape, \" trace:\", np.trace(sigma)) #trace is sum of diagonal elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950b89ab",
   "metadata": {},
   "source": [
    "### 4. regularise Σ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "764ecebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized covariance shape: (64, 64)  trace: 41.14902204977638\n"
     ]
    }
   ],
   "source": [
    "# Σ_λ= Σ + λI\n",
    "lambda_reg = 1e-3\n",
    "sigma_reg = sigma + lambda_reg * np.eye(D)\n",
    "print(\"Regularized covariance shape:\", sigma_reg.shape, \" trace:\", np.trace(sigma_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5867ea",
   "metadata": {},
   "source": [
    "### 5. computing log Gaussian scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6b93987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we assume p(x|y=k) ~ N(x;mu_k, Σ_λ) and p(y=k) = pi_k\n",
    "# What we calculate is posterior score of sample x for class k = log p(y=k) + log p(x|y=k) <------- important\n",
    "# log p(x|y=k) = -0.5 * [ D log(2pi) + log|Σ_λ| + (x - mu_k)^T Σ_λ^-1 (x - mu_k) ] from the multivariate normal distribution\n",
    "# the term -0.5 * D log(2pi)- 0.5 * log|Σ_λ| is constant for all classes, so we can ignore it in the score computation\n",
    "\n",
    "def compute_scores(X):\n",
    "    N= X.shape[0]\n",
    "    D= X.shape[1]\n",
    "    K= mu.shape[0]\n",
    "    scores= np.zeros((N,K))\n",
    "    sign, logdet= slogdet(sigma_reg)\n",
    "    if sign <= 0:\n",
    "        raise ValueError(\"Covariance matrix is not positive definite.\")\n",
    "    sigma_inv= inv(sigma_reg)\n",
    "    const_term= -0.5 * (D * np.log(2*np.pi) + logdet) # constant term for all classes\n",
    "    for k in range(K):\n",
    "        diff= X - mu[k] \n",
    "        for i in range(N):\n",
    "            mahalanobis= diff[i].T.dot(sigma_inv).dot(diff[i])\n",
    "            log_likelihood= const_term - 0.5 * mahalanobis\n",
    "            scores[i,k]= np.log(pi[k]) + log_likelihood\n",
    "    return scores\n",
    "\n",
    "# for each sample in X, compute scores and predict the class with highest score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02652010",
   "metadata": {},
   "source": [
    "### Functions to predict and calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0bfe8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_scores(scores):\n",
    "    return np.argmax(scores, axis=1)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0105be",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning and Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c31bf355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Lambda  Validation Accuracy\n",
      "0   0.0001             0.944444\n",
      "1   0.0010             0.944444\n",
      "2   0.0100             0.944444\n",
      "3   0.1000             0.944444\n",
      "4   1.0000             0.922222\n",
      "5  10.0000             0.848148\n",
      "\n",
      "best lambda: 0.0001 val acc: 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "lambdas= [1e-4, 1e-3, 1e-2, 1e-1, 1, 10]\n",
    "best_acc = -np.inf\n",
    "best_lam = None\n",
    "lambdas_table= []\n",
    "\n",
    "for lambda_reg in lambdas:\n",
    "    sigma_reg = sigma + lambda_reg * np.eye(D)\n",
    "    scores_val= compute_scores(X_val)\n",
    "    y_val_pred= predict_from_scores(scores_val)\n",
    "    acc= accuracy(y_val, y_val_pred)\n",
    "    if acc> best_acc:\n",
    "        best_acc=acc\n",
    "        best_lam=lambda_reg\n",
    "    lambdas_table.append((lambda_reg, acc))\n",
    "    \n",
    "lambdas_df= pd.DataFrame(lambdas_table, columns=['Lambda', 'Validation Accuracy'])\n",
    "print(lambdas_df)\n",
    "print(\"\\nbest lambda:\", best_lam, \"val acc:\", best_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2ebaba",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7e215f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final accuracy on combined train+val set with lambda=0.0001: 96.46%\n"
     ]
    }
   ],
   "source": [
    "# combine train and val sets for final evaluation\n",
    "X_combined= np.vstack((X_train, X_val))\n",
    "y_combined= np.hstack((y_train, y_val))\n",
    "N_combined= X_combined.shape[0]\n",
    "lambda_reg = best_lam\n",
    "sigma_reg = sigma + lambda_reg * np.eye(D)\n",
    "scores_final= compute_scores(X_combined)\n",
    "y_final_pred= predict_from_scores(scores_final)\n",
    "final_acc= accuracy(y_combined, y_final_pred)\n",
    "print(f\"\\nFinal accuracy on combined train+val set with lambda={best_lam}: {final_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef98e93",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe351da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy evaluation:  96.46%\n",
      "Macro averaged precision:  96.52%\n",
      "Macro averaged recall:  96.46%\n",
      "Macro averaged F1-score:  96.48%\n",
      "Confusion Matrix:\n",
      " [[150   0   0   0   0   0   1   0   0   0]\n",
      " [  0 146   1   0   1   0   0   0   2   4]\n",
      " [  0   0 148   3   0   0   0   0   0   0]\n",
      " [  0   0   1 149   0   1   0   0   3   1]\n",
      " [  0   1   0   0 149   0   0   1   2   1]\n",
      " [  0   0   0   0   0 147   1   0   0   7]\n",
      " [  0   2   0   0   1   0 151   0   0   0]\n",
      " [  0   0   0   0   0   0   0 150   0   2]\n",
      " [  0   6   0   0   0   0   0   1 139   2]\n",
      " [  0   0   0   1   0   1   0   2   5 144]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy evaluation: \",f\"{final_acc*100:.2f}%\")\n",
    "print(\"Macro averaged precision: \",f\"{precision_score(y_combined, y_final_pred, average='macro')*100:.2f}%\")\n",
    "print(\"Macro averaged recall: \",f\"{recall_score(y_combined, y_final_pred, average='macro')*100:.2f}%\")\n",
    "print(\"Macro averaged F1-score: \",f\"{f1_score(y_combined, y_final_pred, average='macro')*100:.2f}%\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_combined, y_final_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eed9ef1",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3dfc0602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# short explanation of the Gaussian Generative Model , parameter estimation and regularization.\n",
    "# Gaussian Generative Model:\n",
    "# - We assume each class y=k has a prior probability π_k = p(y=k), estimated from the fraction of training samples in that class.\n",
    "# - The class-conditional distribution p(x | y=k) is multivariate Gaussian: x | y=k ~ N(μ_k, Σ), \n",
    "#   where μ_k is the mean vector for class k and Σ is a shared covariance matrix across all classes.\n",
    "# Parameter estimation:\n",
    "# - π_k: proportion of training samples in class k.\n",
    "# - μ_k: average of all training samples belonging to class k.\n",
    "# - Σ: accumulated outer product of deviations (x_i - μ_{y_i})(x_i - μ_{y_i})^T over all samples, divided by total number of samples.\n",
    "# Regularization:\n",
    "# - To ensure numerical stability and invertibility, we use Σ_λ = Σ + λI.\n",
    "# - λ > 0 prevents singular matrices and smooths the covariance; larger λ produces more “spherical” distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f114af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discussion:\n",
    "# The confusion matrix shows that certain digits are more frequently confused with each other, \n",
    "# particularly visually similar ones. For example, digits 1 and 7, or 3 and 8, are commonly misclassified, \n",
    "# while digits like 0, 4, and 6 are recognized very accurately. These confusions reflect the inherent similarity \n",
    "# in pixel patterns for these digits.\n",
    "\n",
    "# The choice of λ had a clear effect on performance. Small λ values can lead to overfitting, making the model \n",
    "# sensitive to noise in the training set, while very large λ oversmooths the covariance and reduces separation \n",
    "# between classes. Overall, the Gaussian generative model is fast, interpretable, and works well for distinct \n",
    "# digits, but its assumption of a shared covariance limits its ability to capture class-specific feature correlations, \n",
    "# and it struggles with overlapping or non-Gaussian patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d31a163",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
